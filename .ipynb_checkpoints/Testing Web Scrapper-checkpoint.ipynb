{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from proxy_requests import ProxyRequests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "import csv\n",
    "import re\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "global results\n",
    "results = {True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes url of the user search results\n",
    "# ex: https://myanimelist.net/users.php?q=&loc=&agelow=0&agehigh=0&g=1\n",
    "def url_to_soup(url, proxy = False):\n",
    "    url_prefix = 'https://'\n",
    "    if (url_prefix not in url):\n",
    "        url = url_prefix + url\n",
    "    \n",
    "    r = \"\"\n",
    "    isRetry = False\n",
    "    while (str(r) == \"\"):\n",
    "        if (isRetry):\n",
    "            print('Thread:', threading.currentThread().getName())\n",
    "            print(\"Retrying Bad Request\")\n",
    "            \n",
    "        if (proxy):\n",
    "            r = ProxyRequests(url)\n",
    "            r.get()\n",
    "        else:\n",
    "            r = requests.get(url)\n",
    "            \n",
    "        isRetry = True\n",
    "\n",
    "    return BeautifulSoup(str(r), 'html.parser')\n",
    "\n",
    "def soup_to_users(soup):\n",
    "    # content > table > tbody > div > a\n",
    "    users = soup.find_all('a', {\"href\" : re.compile(r\"/profile/*\")})\n",
    "    # filters all users\n",
    "    users = list(filter(lambda elem : elem.get_text() != \"\", users))\n",
    "    users = [elem.get_text() for elem in users]\n",
    "    \n",
    "    return users\n",
    "\n",
    "# hard coded approach that allows multi-threading\n",
    "def base_url_to_pg_link(url, pg):\n",
    "    return (url + str(24 * pg))\n",
    "\n",
    "# useless for multi-threading\n",
    "def url_and_soup_to_pg_link(url, soup, pg):\n",
    "    # takes domain of url\n",
    "    url = url.replace(\"https://\",\"\")\n",
    "    url = url.replace(\"www.\", \"\")\n",
    "    url = url.split('/')[0]\n",
    "    \n",
    "    # think about optimizing this line so it doesn't have to iterate twice\n",
    "    pg_links = soup.find_all('a', {\"href\" : re.compile(r\"/users.php*\")})\n",
    "    url_path = [link['href'] for link in pg_links \n",
    "                if link.get_text() == str(pg)][0]\n",
    "    \n",
    "    return (url + url_path)\n",
    "\n",
    "# relies on a valid list called results\n",
    "def store_scrape_results(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        results.add(func(*args, **kwargs))\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# scrapes and inputs users\n",
    "# returns false when no users left\n",
    "# pg starts index at 0\n",
    "@store_scrape_results\n",
    "def scrape_users(user_list, base_url, pg, is_proxy = False):\n",
    "    url = base_url_to_pg_link(base_url, pg)\n",
    "    soup = url_to_soup(url, proxy = is_proxy)\n",
    "    users = soup_to_users(soup)\n",
    "    if (len(users) == 0):\n",
    "        return False\n",
    "    user_list.extend(users)\n",
    "    return True\n",
    "\n",
    "def scrape_users_in_gender(base_url, fn, pg_cap = 30000, cap_threads = 32):\n",
    "    tot_users = list()\n",
    "    results = {True}\n",
    "    pg_num = 0\n",
    "\n",
    "    print(\"Start Scraping\")\n",
    "    start_threads = threading.activeCount()\n",
    "    try:\n",
    "        while (pg_num < pg_cap and False not in results):\n",
    "            # have cap_threads number of threads running at a time to complete task\n",
    "            while ((threading.activeCount() - start_threads) < cap_threads and \n",
    "                   False not in results):\n",
    "\n",
    "                t = threading.Thread(\n",
    "                    target = scrape_users,\n",
    "                    kwargs = {\n",
    "                        'user_list' : tot_users, \n",
    "                        'base_url' : m_base_url, \n",
    "                        'pg' : pg_num, \n",
    "                        'is_proxy' : True,\n",
    "                    }\n",
    "                )\n",
    "                t.start()\n",
    "\n",
    "                pg_num += 1\n",
    "                \n",
    "                if (pg_num % 100 == 0):\n",
    "                    print(\"Current Page:\", pg_num)\n",
    "                if (pg_num % 1000 == 0 and len(tot_users) != 0):\n",
    "                    print(\"Current Last 100 Users:\", tot_users[-100:])\n",
    "\n",
    "        while (threading.activeCount() != start_threads):\n",
    "            pass\n",
    "        \n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        csv_write(users, fn = 'temp_users.csv')\n",
    "    \n",
    "    print(\"End Scraping\")\n",
    "    print(\"Inputting Users...\")\n",
    "    csv_write(tot_users, fn)\n",
    "    print(\"Finished\")\n",
    "\n",
    "def csv_write(users, fn):\n",
    "    with open(fn, 'w', newline = '') as open_test:\n",
    "        my_csv = csv.writer(open_test)\n",
    "        for user in users:\n",
    "            my_csv.writerow([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base_url = 'https://myanimelist.net/users.php?q=&loc=&agelow=0&agehigh=0&g=1&show='\n",
    "f_base_url = 'https://myanimelist.net/users.php?q=&loc=&agelow=0&agehigh=0&g=2&show='\n",
    "\n",
    "scrape_users_in_gender(m_base_url, fn = 'm_users.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
